{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.112.213:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb576ee2f28>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker_pyspark\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "# See the SageMaker Spark Github to learn how to connect to EMR from a notebook instance\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\\\n",
    "    .master(\"local[*]\").getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.functions import col, udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "file =requests.get('https://s3.amazonaws.com/mrinal-ml-s2005_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year1 = spark.read.csv('2005_data.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[resident_status: string, education_1989_revision: string, education_2003_revision: string, education_reporting_flag: string, month_of_death: string, sex: string, detail_age_type: string, detail_age: string, age_substitution_flag: string, age_recode_52: string, age_recode_27: string, age_recode_12: string, infant_age_recode_22: string, place_of_death_and_decedents_status: string, marital_status: string, day_of_week_of_death: string, current_data_year: string, injury_at_work: string, manner_of_death: string, method_of_disposition: string, autopsy: string, activity_code: string, place_of_injury_for_causes_w00_y34_except_y06_and_y07_: string, icd_code_10th_revision: string, 358_cause_recode: string, 113_cause_recode: string, 130_infant_cause_recode: string, 39_cause_recode: string, number_of_entity_axis_conditions: string, entity_condition_1: string, entity_condition_2: string, entity_condition_3: string, entity_condition_4: string, entity_condition_5: string, entity_condition_6: string, entity_condition_7: string, entity_condition_8: string, entity_condition_9: string, entity_condition_10: string, entity_condition_11: string, entity_condition_12: string, entity_condition_13: string, entity_condition_14: string, entity_condition_15: string, entity_condition_16: string, entity_condition_17: string, entity_condition_18: string, entity_condition_19: string, entity_condition_20: string, number_of_record_axis_conditions: string, record_condition_1: string, record_condition_2: string, record_condition_3: string, record_condition_4: string, record_condition_5: string, record_condition_6: string, record_condition_7: string, record_condition_8: string, record_condition_9: string, record_condition_10: string, record_condition_11: string, record_condition_12: string, record_condition_13: string, record_condition_14: string, record_condition_15: string, record_condition_16: string, record_condition_17: string, record_condition_18: string, record_condition_19: string, record_condition_20: string, race: string, bridged_race_flag: string, race_imputation_flag: string, race_recode_3: string, race_recode_5: string, hispanic_origin: string, hispanic_originrace_recode: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|record_condition_5|  count|\n",
      "+------------------+-------+\n",
      "|              null|2115768|\n",
      "|              J449|  21238|\n",
      "|              I500|  16352|\n",
      "|               N19|  15108|\n",
      "|              J969|  14180|\n",
      "|              J189|  10526|\n",
      "|              I469|  10113|\n",
      "|               I64|   8448|\n",
      "|              I251|   8397|\n",
      "|              N189|   7584|\n",
      "|              N179|   6790|\n",
      "|              R092|   6750|\n",
      "|               I48|   6029|\n",
      "|              N390|   5644|\n",
      "|              I709|   4559|\n",
      "|               W80|   4192|\n",
      "|               I10|   4171|\n",
      "|              R628|   3859|\n",
      "|              J690|   3792|\n",
      "|               X59|   3721|\n",
      "+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "year1.groupBy('record_condition_5').count().orderBy('count', ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
